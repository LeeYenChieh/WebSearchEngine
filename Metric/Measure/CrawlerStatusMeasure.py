from Metric.Measure.Measure import Measure
from Database.Database import Database
from Database.ModelFactory.AppModelFactory import AppModelFactory
from datetime import datetime, timedelta, date
from sqlalchemy import func, case, select, and_
from sqlalchemy.dialects.postgresql import insert
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from collections import defaultdict

class CrawlerStatusMeasure(Measure):
    def __init__(self, modelFactory: AppModelFactory, crawlerDB: Database, metricDB: Database):
        super().__init__()
        self.crawlerDB: Database = crawlerDB
        self.metricDB: Database = metricDB
        self.modelFactory: AppModelFactory = modelFactory
    
    def _scan_shard(self, shard_id):
        """
        æŽƒæå–®ä¸€åˆ†ç‰‡ï¼Œå–å¾—ç›®å‰çš„ Snapshot ç‹€æ…‹
        """
        with self.crawlerDB.session() as session:
            try:
                UrlState = self.modelFactory.create_url_state_model(shard_id)
                
                stmt = select(
                    func.count(), # discovered
                    func.count(case((UrlState.fetch_ok > 0, 1))), # crawled
                    func.count(case((UrlState.indexed > 0, 1)))   # indexed
                )
                
                result = session.execute(stmt).one()
                return shard_id, result[0], result[1], result[1]
                
            except Exception as e:
                return shard_id, 0, 0, 0

    def _get_daily_summary_stats(self, target_date: date):
        """
        å¾ž SummaryDaily è¨ˆç®— 1å¤©, 7å¤©, 30å¤© çš„çµ±è¨ˆæ•¸æ“š
        å›žå‚³ dict: åŒ…å«æ‰€æœ‰ CrawlerStatMixin éœ€è¦çš„ fetch èˆ‡ error æ¬„ä½
        """
        stats_data = defaultdict(int)
        
        # å®šç¾©æ™‚é–“ç¯„åœ
        start_date_30 = target_date - timedelta(days=29) # å«ä»Šå¤©å…±30å¤©
        
        SummaryDaily = self.modelFactory.create_summary_model()
        
        with self.crawlerDB.session() as session:
            # ä¸€æ¬¡æ’ˆå–éŽåŽ» 30 å¤©çš„è³‡æ–™
            rows = session.query(SummaryDaily).filter(
                and_(SummaryDaily.stat_date >= start_date_30, SummaryDaily.stat_date <= target_date)
            ).all()
            
            # åœ¨è¨˜æ†¶é«”ä¸­é€²è¡Œèšåˆè¨ˆç®— (Python loop)
            # é€™æ¨£è™•ç† JSONB çš„ key ç´¯åŠ æ¯”å¯«è¤‡é›œ SQL å®¹æ˜“ç¶­è­·
            for row in rows:
                # è¨ˆç®—è©²åˆ—èˆ‡ç›®æ¨™æ—¥æœŸçš„å·®è·
                delta_days = (target_date - row.stat_date).days
                
                # åŸºç¤Žæ•¸å€¼
                f_ok = row.fetch_ok or 0
                f_fail = row.fetch_fail or 0
                f_total = f_ok + f_fail
                
                # è§£æž fail_reasons (JSONB)
                reasons = row.fail_reasons if row.fail_reasons else {}
                err_404 = reasons.get("HttpError 404", 0)
                err_500 = reasons.get("HttpError 500", 0) # ä¾æ“šä½ çš„éœ€æ±‚ HttpError 505
                
                # 1. ç•¶æ—¥æ•¸æ“š (delta_days == 0)
                if delta_days == 0:
                    stats_data["fetch_ok"] = f_ok
                    stats_data["fetch_fail"] = f_fail
                    stats_data["fetch_total"] = f_total
                    stats_data["http_error_404"] = err_404
                    stats_data["http_error_500"] = err_500

                # 2. 7å¤©æ»¾å‹•æ•¸æ“š (0 <= delta_days < 7)
                if delta_days < 7:
                    stats_data["fetch_ok_7"] += f_ok
                    stats_data["fetch_fail_7"] += f_fail
                    stats_data["fetch_total_7"] += f_total
                    stats_data["http_error_404_7"] += err_404
                    stats_data["http_error_500_7"] += err_500

                # 3. 30å¤©æ»¾å‹•æ•¸æ“š (0 <= delta_days < 30)
                if delta_days < 30:
                    stats_data["fetch_ok_30"] += f_ok
                    stats_data["fetch_fail_30"] += f_fail
                    stats_data["fetch_total_30"] += f_total
                    stats_data["http_error_404_30"] += err_404
                    stats_data["http_error_500_30"] += err_500
                    
        return stats_data

    def test(self):
        # è¨­å®šæ—¥æœŸ (ä½¿ç”¨ date ç‰©ä»¶)
        now = datetime.now()
        today_date = now.date()
        date_str = now.strftime('%Y-%m-%d')
        
        # 1. åˆå§‹åŒ– Snapshot çµ±è¨ˆå®¹å™¨
        # é€™äº›æ˜¯å¾ž url_state ç®—å‡ºä¾†çš„ç´¯è¨ˆå€¼ (Total Snapshot)
        snapshot_stats = {
            "Total":   {"discovered": 0, "crawled": 0, "indexed": 0},
            "A":       {"discovered": 0, "crawled": 0, "indexed": 0}, # 000-127
            "B":       {"discovered": 0, "crawled": 0, "indexed": 0}  # 128-255
        }

        print(f'ðŸš€ Start Measuring Status - {date_str}')
        
        # 2. [Snapshot] å¹³è¡ŒæŽƒæ Shards
        print("   [1/3] Scanning UrlState Shards...")
        MAX_WORKERS = 16
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(self._scan_shard, i) for i in range(256)]
            
            for future in tqdm(as_completed(futures), total=256, desc="Scanning Shards"):
                shard_id, disc, crawl, idx = future.result()
                
                # åˆ†é¡ž Team A / Team B
                if 0 <= shard_id <= 127:
                    team_key = "A"
                else:
                    team_key = "B"
                
                # ç´¯åŠ  (Team & Total)
                for key, val in [("discovered", disc), ("crawled", crawl), ("indexed", idx)]:
                    snapshot_stats[team_key][key] += val
                    snapshot_stats["Total"][key] += val

        # 3. [Flow] è¨ˆç®— SummaryDaily çµ±è¨ˆ (Fetch & Errors & Rolling)
        print("   [2/3] Calculating Daily & Rolling Stats...")
        daily_flow_stats = self._get_daily_summary_stats(today_date)
        
        # 4. å¯«å…¥ MetricDB
        print(f"   [3/3] Saving to MetricDB...")
        
        suffixes = ["Total", "A", "B"]

        with self.metricDB.session() as session:
            for suffix in suffixes:
                ModelClass = self.modelFactory.create_crawler_stat_model(suffix)
                
                # æº–å‚™åŸºç¤Žè³‡æ–™ (ä¾†è‡ª Snapshot æŽƒæ)
                row_data = {
                    "stat_date": today_date,
                    "discovered": snapshot_stats[suffix]["discovered"],
                    "crawled":    snapshot_stats[suffix]["crawled"],
                    "indexed":    snapshot_stats[suffix]["indexed"],
                }
                
                # å¦‚æžœæ˜¯ Total è¡¨ï¼Œæˆ‘å€‘è¦å¡«å…¥å®Œæ•´çš„ SummaryDaily çµ±è¨ˆæ•¸æ“š
                # (å› ç‚º SummaryDaily æ˜¯å…¨åŸŸçš„ï¼Œç„¡æ³•æ‹†åˆ† A/Bï¼Œé™¤éžæœ‰é¡å¤–é‚è¼¯)
                if suffix == "Total":
                    row_data.update(daily_flow_stats)
                else:
                    # å°æ–¼ A å’Œ Bï¼Œè‹¥ç„¡æ•¸æ“šä¾†æºï¼Œå‰‡è£œ 0 ä»¥é˜²è³‡æ–™åº« Null constraint (å¦‚æžœæ¬„ä½æ²’è¨­ default)
                    # æˆ–æ˜¯ç¶­æŒ None è®“ DB default é‹ä½œã€‚
                    # é€™è£¡å‡è¨­æˆ‘å€‘è¦é¡¯å¼å¡« 0 æˆ–æ˜¯åªå¡« snapshotã€‚
                    # æ ¹æ“šéœ€æ±‚ï¼šA, B çš„ fetch_ok ç­‰æ¬„ä½è‹¥ç„¡æ³•è¨ˆç®—ï¼Œé€šå¸¸å¡« 0ã€‚
                    keys_to_zero = [
                        "fetch_ok", "fetch_fail", "fetch_total",
                        "fetch_ok_7", "fetch_fail_7", "fetch_total_7",
                        "fetch_ok_30", "fetch_fail_30", "fetch_total_30",
                        "http_error_404", "http_error_404_7", "http_error_404_30",
                        "http_error_500", "http_error_500_7", "http_error_500_30"
                    ]
                    for k in keys_to_zero:
                        row_data[k] = 0

                # åŸ·è¡Œ Upsert
                stmt = insert(ModelClass).values(row_data)
                stmt = stmt.on_conflict_do_update(
                    index_elements=['stat_date'],
                    set_=row_data
                )
                
                session.execute(stmt)
            
            session.commit()

        # 5. è¼¸å‡ºå ±å‘Š
        self._print_report(date_str, snapshot_stats, daily_flow_stats)

    def _print_report(self, date_str, snapshot_stats, flow_stats):
        print("\n" + "="*50)
        print(f"ðŸ“Š Crawler Status Report: {date_str}")
        print("="*50)
        
        # Snapshot Report
        headers = f"{'Group':<8} | {'Discovered':>12} | {'Crawled':>12} | {'Indexed':>12}"
        print(headers)
        print("-" * len(headers))
        for group in ["A", "B", "Total"]:
            d = snapshot_stats[group]
            print(f"{group:<8} | {d['discovered']:>12,} | {d['crawled']:>12,} | {d['indexed']:>12,}")
            
        print("-" * 50)
        # Flow Report (Total Only)
        print(f"Daily Flow (Total):")
        print(f"  Fetch OK (Today):      {flow_stats['fetch_ok']:,}")
        print(f"  Fetch OK (7 Days):     {flow_stats['fetch_ok_7']:,}")
        print(f"  Fetch OK (30 Days):    {flow_stats['fetch_ok_30']:,}")
        print(f"  404 Errors (7 Days):   {flow_stats['http_error_404_7']:,}")
        print("="*50)