from Metric.Measure.Measure import Measure
from Metric.Dataset.Dataset import Dataset
from Database.Database import Database
from sqlalchemy import text
from tqdm import tqdm
from collections import defaultdict

class CrawlerAllMetricMeasure(Measure):
    def __init__(self, dataset: Dataset, db: Database, resultDataset: Dataset):
        super().__init__()
        self.dataset = dataset
        self.db: Database = db
        self.resultDataset = resultDataset
        self.resultDataset.clear()
    
    def test(self):
        print(f'Start Measuring Crawler (Direct DB), data: {self.dataset.path}')

        # 1. æ”¶é›†æ‰€æœ‰çš„ Golden URL (å»é‡ï¼ŒåŠ é€ŸæŸ¥è©¢)
        all_golden_urls = set()
        for keyword in self.dataset.getKeys():
            urls = self.dataset.get(keyword).get('url', [])
            for u in urls:
                all_golden_urls.add(u)
        
        # è½‰æ›æˆ List ä»¥ä¾¿ SQL ä½¿ç”¨
        url_list = list(all_golden_urls)
        total_urls_count = len(url_list)
        
        # 2. åˆå§‹åŒ–ç‹€æ…‹å­—å…¸
        # key: url, value: {discovered: bool, crawled: bool, indexed: bool, table_id: int}
        url_status_map = {
            u: {'discovered': False, 'crawled': False, 'indexed': False, 'table_id': -1} 
            for u in url_list
        }

        # 3. éæ­·è³‡æ–™åº« (url_state_000 ~ url_state_255)
        # ç‚ºäº†æ•ˆç‡ï¼Œæˆ‘å€‘ä¸ä¸€ç­†ä¸€ç­†æŸ¥ï¼Œè€Œæ˜¯æ¯å¼µè¡¨æŸ¥ä¸€æ¬¡ "é€™å¼µè¡¨è£¡æœ‰æ²’æœ‰æˆ‘å€‘çš„ Golden URL"
        print("Scanning 256 Database Tables...")
        
        # å¦‚æœ URL å¾ˆå¤š (ä¾‹å¦‚ > 5000)ï¼Œå»ºè­°åˆ†æ‰¹åˆ‡åˆ† url_list æ”¾å…¥ IN å­å¥ï¼Œé€™è£¡å‡è¨­æ¸¬è©¦é›†ä¸å¤§ç›´æ¥å¡
        
        with self.db.session() as session:
            # ä½¿ç”¨ tqdm é¡¯ç¤ºæƒæé€²åº¦
            for i in tqdm(range(256), desc="Checking Tables"):
                table_name = f'url_state_{i:03}'
                
                # å»ºæ§‹ SQL: åªæ’ˆå–æ˜¯ Golden URL çš„è³‡æ–™
                # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ text() åŸ·è¡Œ Raw SQL ä»¥ç²å¾—æœ€é«˜æ•ˆèƒ½ï¼Œé¿å… ORM overhead
                sql = text(f"""
                    SELECT url, fetch_ok, indexed 
                    FROM {table_name} 
                    WHERE url IN :urls
                """)
                
                # åŸ·è¡ŒæŸ¥è©¢
                try:
                    result = session.execute(sql, {'urls': tuple(url_list)}).fetchall()
                    
                    # æ›´æ–°ç‹€æ…‹
                    for row in result:
                        url = row[0]
                        fetch_ok = row[1]
                        indexed = row[2]
                        
                        if url in url_status_map:
                            url_status_map[url]['discovered'] = True
                            url_status_map[url]['crawled'] = (fetch_ok > 0)
                            url_status_map[url]['indexed'] = (indexed == 1)
                            url_status_map[url]['table_id'] = i
                            
                except Exception as e:
                    # æœ‰äº›è¡¨å¯èƒ½é‚„æ²’å»ºç«‹ï¼Œå¯ä»¥å¿½ç•¥æˆ– print
                    # print(f"Skipping {table_name}: {e}")
                    pass

        # 4. çµ±è¨ˆåˆ†æ•¸ (Global, Group A, Group B)
        stats = {
            'total': {'discover': 0, 'fetch': 0, 'upload': 0},
            'group_a': {'discover': 0, 'fetch': 0, 'upload': 0}, # 000-127
            'group_b': {'discover': 0, 'fetch': 0, 'upload': 0}  # 128-255
        }

        # ç”¨ä¾†è¨ˆç®— resultDataset çš„é‚è¼¯
        for keyword in tqdm(self.dataset.getKeys(), desc="Aggregating Results"):
            keyword_results = []
            
            for goldenurl in self.dataset.get(keyword)['url']:
                status = url_status_map.get(goldenurl, {})
                
                is_discovered = status.get('discovered', False)
                is_crawled = status.get('crawled', False)
                is_indexed = status.get('indexed', False)
                table_id = status.get('table_id', -1)
                
                # å­˜å…¥ Result Dataset
                keyword_results.append({
                    'url': goldenurl,
                    'discover_find': is_discovered,
                    'fetch_find': is_crawled,
                    'upload_find': is_indexed
                })

                # æ›´æ–°çµ±è¨ˆæ•¸æ“š
                if is_discovered:
                    # Global
                    stats['total']['discover'] += 1
                    if is_crawled: stats['total']['fetch'] += 1
                    if is_indexed: stats['total']['upload'] += 1
                    
                    # Grouping
                    if 0 <= table_id <= 127:
                        stats['group_a']['discover'] += 1
                        if is_crawled: stats['group_a']['fetch'] += 1
                        if is_indexed: stats['group_a']['upload'] += 1
                    elif 128 <= table_id <= 255:
                        stats['group_b']['discover'] += 1
                        if is_crawled: stats['group_b']['fetch'] += 1
                        if is_indexed: stats['group_b']['upload'] += 1

            # å„²å­˜å€‹åˆ¥ Keyword çš„çµæœ
            if self.resultDataset.get(keyword) is None:
                self.resultDataset.store(keyword, keyword_results)
            else:
                self.resultDataset.get(keyword).extend(keyword_results)

        # 5. è¼¸å‡ºçµæœ
        total_golden_count = sum(len(self.dataset.get(k)['url']) for k in self.dataset.getKeys())
        
        print("\n" + "="*30)
        print(f"ğŸ“Š Evaluation Report (Total Golden URLs: {total_golden_count})")
        print("="*30)

        # Print Group A (000-127)
        print(f"[Group A (Tables 000-127)] Found:")
        print(f"  - Discovered: {stats['group_a']['discover']}")
        print(f"  - Crawled:    {stats['group_a']['fetch']}")
        print(f"  - Indexed:    {stats['group_a']['upload']}")
        print("-" * 30)

        # Print Group B (128-255)
        print(f"[Group B (Tables 128-255)] Found:")
        print(f"  - Discovered: {stats['group_b']['discover']}")
        print(f"  - Crawled:    {stats['group_b']['fetch']}")
        print(f"  - Indexed:    {stats['group_b']['upload']}")
        print("-" * 30)

        # Print Total
        print(f"[Total Performance]")
        print(f"  - Discovered: {stats['total']['discover']} / {total_golden_count} ({stats['total']['discover']/total_golden_count:.2%})")
        print(f"  - Fetch:      {stats['total']['fetch']} / {total_golden_count} ({stats['total']['fetch']/total_golden_count:.2%})")
        print(f"  - Upload:     {stats['total']['upload']} / {total_golden_count} ({stats['total']['upload']/total_golden_count:.2%})")
        print("="*30)

        # å„²å­˜ç¸½çµåˆ° JSON
        self.resultDataset.store("__total__", {
            "discover_find": stats['total']['discover'],
            "fetch_find": stats['total']['fetch'],
            "upload_find": stats['total']['upload'],
            "total": total_golden_count,
            "group_a_stats": stats['group_a'],
            "group_b_stats": stats['group_b']
        })
        self.resultDataset.dump()