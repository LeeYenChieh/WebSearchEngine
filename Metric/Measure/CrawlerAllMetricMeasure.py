from Metric.Measure.Measure import Measure
from Database.Database import Database
from Database.ModelFactory.AppModelFactory import AppModelFactory
from sqlalchemy import select, func, and_
from sqlalchemy.dialects.postgresql import insert
from tqdm import tqdm
import tldextract
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

class CrawlerAllMetricMeasure(Measure):
    def __init__(self, modelFactory: AppModelFactory, crawlerDB: Database, metricDB: Database, batch_id: int, tag: str):
        """
        åˆå§‹åŒ– CrawlerAllMetricMeasure
        :param modelFactory: æ¨¡å‹å·¥å» 
        :param crawlerDB: çˆ¬èŸ²è³‡æ–™åº« (è®€å– Shard ç‹€æ…‹)
        :param metricDB: æŒ‡æ¨™è³‡æ–™åº« (è®€å– Golden URL / å¯«å…¥è¦†è“‹ç‡)
        :param batch_id: æŒ‡å®šè¦è©•ä¼°çš„ MetricBatch ID
        :param tag: æŒ‡å®š Metric æ¨™ç±¤ (ä¾‹å¦‚ 'head', 'random')ï¼Œç”¨ä¾†ç¯©é¸ Golden URLs
        """
        super().__init__()
        self.modelFactory = modelFactory
        self.crawlerDB: Database = crawlerDB
        self.metricDB: Database = metricDB
        self.batch_id = batch_id
        self.tag = tag
        
        # åˆå§‹åŒ– TLD Extractor (é—œé–‰å¿«å–é¿å…æ¬Šé™å•é¡Œ)
        self.extractor = tldextract.TLDExtract(cache_dir=False)
    
    def get_domain(self, url: str) -> str:
        try:
            extracted = self.extractor(url)
            if extracted.domain and extracted.suffix:
                return f"{extracted.domain}.{extracted.suffix}"
            return ""
        except:
            return ""

    def _scan_domain_shard(self, shard_ids, domain_tuple):
        """
        æƒæ CrawlerDB çš„ Domain Tablesï¼Œå»ºç«‹ Domain -> Shard ID çš„æ˜ å°„
        """
        found_map = {}
        with self.crawlerDB.session() as session:
            for i in shard_ids:
                try:
                    DomainStats = self.modelFactory.create_domain_stats_model(i)
                    stmt = select(DomainStats.domain).where(DomainStats.domain.in_(domain_tuple))
                    result = session.execute(stmt).scalars()
                    for domain in result:
                        found_map[domain] = i
                except Exception:
                    pass
        return found_map

    def _scan_url_shard(self, shard_ids, url_tuple):
        """
        æƒæ CrawlerDB çš„ Url State Tablesï¼Œå–å¾— URL çš„ç™¼ç¾/çˆ¬å–/ç´¢å¼•ç‹€æ…‹
        """
        found_data = [] # List of (url, fetch_ok, indexed, table_id)
        
        with self.crawlerDB.session() as session:
            for i in shard_ids:
                try:
                    UrlState = self.modelFactory.create_url_state_model(i)
                    # åˆ¤æ–· URL æ˜¯å¦å­˜åœ¨æ–¼è©²åˆ†ç‰‡
                    stmt = select(
                        UrlState.url, 
                        UrlState.fetch_ok, 
                        UrlState.indexed
                    ).where(UrlState.url.in_(url_tuple))

                    result = session.execute(stmt)
                    for row in result:
                        found_data.append((row.url, row.fetch_ok, row.indexed, i))
                except Exception:
                    pass
        return found_data

    def test(self):
        """
        ä¸»åŸ·è¡Œé‚è¼¯ (ä½¿ç”¨ __init__ å‚³å…¥çš„ batch_id å’Œ tag)
        """
        print(f'ğŸš€ Start Measuring Crawler Coverage (Batch: {self.batch_id}, Tag: {self.tag})')
        today_date = datetime.now().date()
        
        # å°æ‡‰ MetricCoverage çš„ Set Type (ä¾‹å¦‚ "head" -> "HeadSet")
        set_type = f"{self.tag.capitalize()}Set"

        # ==========================================
        # 1. å¾ MetricDB è®€å– Golden URLs
        # ==========================================
        MetricURL = self.modelFactory.create_metric_url()
        MetricQuery = self.modelFactory.create_metric_queries()
        
        # çµæ§‹: url_str -> list of MetricURL ID
        url_id_map = {} 
        all_golden_domains = set()
        
        print(f"ğŸ“¥ Loading Golden URLs with tag '{self.tag}'...")
        with self.metricDB.session() as session:
            # é€é Join ç¯©é¸ï¼š
            # 1. MetricQuery.batch_id ç¬¦åˆ
            # 2. MetricQuery.tags åŒ…å«æŒ‡å®šçš„ tag (ä½¿ç”¨ JSONB @> æ“ä½œç¬¦)
            stmt = select(MetricURL)\
                .join(MetricQuery)\
                .where(
                    and_(
                        MetricQuery.batch_id == self.batch_id,
                        MetricQuery.tags.contains([self.tag])
                    )
                )
            
            
            results = session.execute(stmt).scalars().all()
            
            if not results:
                print(f"âš ï¸ No URLs found for Batch {self.batch_id} with tag '{self.tag}'. Exiting.")
                return

            for m_url in results:
                url_str = m_url.url
                
                if url_str not in url_id_map:
                    url_id_map[url_str] = []
                
                url_id_map[url_str].append(m_url.id)

                domain = self.get_domain(url_str)
                if domain:
                    all_golden_domains.add(domain)

        # æº–å‚™æƒæç”¨çš„ Tuple
        url_list = list(url_id_map.keys())
        domain_list = list(all_golden_domains)
        url_tuple = tuple(url_list)
        domain_tuple = tuple(domain_list)
        
        print(f"   Loaded {len(url_list)} unique URLs from {len(domain_list)} domains.")

        # ==========================================
        # 2. ä¸¦è¡Œæƒæ CrawlerDB (Shards)
        # ==========================================
        MAX_WORKERS = 16
        chunk_size = 256 // MAX_WORKERS + 1
        shard_chunks = [range(i, min(i + chunk_size, 256)) for i in range(0, 256, chunk_size)]

        # A. æƒæ Domain Tables (ç‚ºäº†ç¢ºå®š Shard ID / Team)
        domain_shard_map = {}
        print(f"ğŸ” Scanning Domain Tables ({MAX_WORKERS} threads)...")
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(self._scan_domain_shard, chunk, domain_tuple) for chunk in shard_chunks]
            for future in tqdm(as_completed(futures), total=len(futures), desc="Domains"):
                domain_shard_map.update(future.result())

        # B. æƒæ URL Tables (ç‚ºäº†ç¢ºå®š Status)
        # åˆå§‹åŒ– URL ç‹€æ…‹
        url_status_map = {
            u: {'discovered': False, 'crawled': False, 'indexed': False, 'shard_id': -1} 
            for u in url_list
        }
        
        print(f"ğŸ” Scanning URL Tables ({MAX_WORKERS} threads)...")
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(self._scan_url_shard, chunk, url_tuple) for chunk in shard_chunks]
            for future in tqdm(as_completed(futures), total=len(futures), desc="URLs"):
                results = future.result()
                for (url, fetch_ok, indexed, shard_id) in results:
                    if url in url_status_map:
                        url_status_map[url]['discovered'] = True
                        url_status_map[url]['crawled'] = (fetch_ok > 0)
                        url_status_map[url]['indexed'] = (indexed == 1)
                        url_status_map[url]['shard_id'] = shard_id

        # ==========================================
        # 3. èšåˆçµ±è¨ˆèˆ‡åˆ†çµ„ (Team A / Team B)
        # ==========================================
        print("ğŸ”„ Aggregating Stats...")
        
        # çµ±è¨ˆå®¹å™¨ï¼šåˆ†åˆ¥çµ±è¨ˆ Total, Team A, Team B
        stats = {
            "Total": {"total": 0, "disc": 0, "crawl": 0, "idx": 0},
            "A":     {"total": 0, "disc": 0, "crawl": 0, "idx": 0},
            "B":     {"total": 0, "disc": 0, "crawl": 0, "idx": 0},
        }
        
        bulk_update_mappings = []

        for url_str, ids in url_id_map.items():
            status = url_status_map.get(url_str, {})
            
            is_disc = status.get('discovered', False)
            is_crawl = status.get('crawled', False)
            is_idx = status.get('indexed', False)
            shard_id = status.get('shard_id', -1)
            
            # å¦‚æœ URL table æ²’æ‰¾åˆ°ï¼Œå˜—è©¦ç”¨ Domain table æ‰¾ Team
            if shard_id == -1:
                domain = self.get_domain(url_str)
                shard_id = domain_shard_map.get(domain, -1)

            # åˆ¤æ–· Team
            team_key = None
            if 0 <= shard_id <= 127:
                team_key = "A"
            elif 128 <= shard_id <= 255:
                team_key = "B"
            
            # æº–å‚™æ‰¹é‡æ›´æ–° MetricURL çš„è³‡æ–™
            for m_id in ids:
                bulk_update_mappings.append({
                    "id": m_id,
                    "is_discovered": is_disc,
                    "is_crawled": is_crawl,
                    "is_indexed": is_idx,
                    "shard_id": shard_id # è¨˜éŒ„æ‰¾åˆ°çš„ shardï¼Œ-1 è¡¨ç¤ºæœªæ‰¾åˆ°
                })

            # ç´¯åŠ çµ±è¨ˆ (Team A/B èˆ‡ Total)
            target_groups = ["Total"]
            if team_key:
                target_groups.append(team_key)
            
            for g in target_groups:
                stats[g]["total"] += 1
                if is_disc: stats[g]["disc"] += 1
                if is_crawl: stats[g]["crawl"] += 1
                if is_idx:  stats[g]["idx"] += 1

        # ==========================================
        # 4. å¯«å…¥ MetricDB
        # ==========================================
        print(f"ğŸ’¾ Saving results to MetricDB...")
        
        with self.metricDB.session() as session:
            # A. æ›´æ–° MetricURL è©³ç´°ç‹€æ…‹
            if bulk_update_mappings:
                session.bulk_update_mappings(MetricURL, bulk_update_mappings)
            
            # B. å¯«å…¥ MetricCoverage çµ±è¨ˆè¡¨ (Total, A, B)
            suffixes = ["Total", "A", "B"]
            
            for suffix in suffixes:
                d = stats[suffix]
                total_count = d["total"]
                
                try:
                    ModelClass = self.modelFactory.create_metric_coverage_model(set_type, suffix)
                except Exception as e:
                    print(f"   [Warning] Could not create model for {set_type}_{suffix}: {e}")
                    continue

                row_data = {
                    "stat_date": today_date,
                    "total": total_count,
                    "discovered_num": d["disc"],
                    "discovered_rate": d["disc"] / total_count if total_count > 0 else 0.0,
                    "crawled_num": d["crawl"],
                    "crawled_rate": d["crawl"] / total_count if total_count > 0 else 0.0,
                    "indexed_num": d["idx"],
                    "indexed_rate": d["idx"] / total_count if total_count > 0 else 0.0,
                    "ranked_num": 0,
                    "ranked_rate": 0.0
                }

                stmt = insert(ModelClass).values(row_data)
                stmt = stmt.on_conflict_do_update(
                    index_elements=['stat_date'],
                    set_=row_data
                )
                session.execute(stmt)
            
            session.commit()
            print("âœ… MetricDB Updated Successfully.")

        # ==========================================
        # 5. è¼¸å‡ºå ±å‘Š
        # ==========================================
        self._print_report(stats)

    def _print_report(self, stats):
        print("\n" + "="*60)
        print(f"ğŸ“Š Coverage Report - Tag: {self.tag}")
        print("="*60)
        
        headers = f"{'Group':<12} | {'Total':>8} | {'Disc %':>10} | {'Crawl %':>10} | {'Index %':>10}"
        print(headers)
        print("-" * len(headers))
        
        groups = ["Total", "A", "B"]
        
        for g in groups:
            d = stats[g]
            total = d['total']
            if total == 0:
                print(f"{g:<12} | {0:>8} | {'N/A':>10} | {'N/A':>10} | {'N/A':>10}")
                continue
                
            disc_rate = d['disc'] / total
            crawl_rate = d['crawl'] / total
            idx_rate = d['idx'] / total
            
            print(f"{g:<12} | {total:>8,} | {disc_rate:>9.1%} | {crawl_rate:>9.1%} | {idx_rate:>9.1%}")
        
        print("="*60)